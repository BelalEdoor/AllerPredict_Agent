# üöÄ AllerPredict AI - Deployment Guide with Ollama

## Table of Contents
1. [Deploy Options with Ollama](#deploy-options-with-ollama)
2. [Quick VPS Deploy (Recommended)](#quick-vps-deploy-recommended)
3. [Docker Deploy](#docker-deploy)
4. [Cloud Deploy (AWS/GCP)](#cloud-deploy)
5. [Production Setup](#production-setup)

---

## ‚ö†Ô∏è Important: Ollama Deployment Considerations

**Ollama requires:**
- Minimum **8GB RAM** (16GB recommended)
- **4+ CPU cores** 
- **20GB+ disk space** for models
- Linux/macOS server (no Windows native support)

**This means:**
- ‚ùå Cannot use free services like Render/Heroku (insufficient RAM)
- ‚úÖ Need VPS (DigitalOcean, Hetzner, Linode)
- ‚úÖ Or AWS EC2, Google Cloud Compute

---

## Deploy Options with Ollama

### Option 1: VPS (Best for Ollama) üí∞ ~$12-40/month
**Recommended Providers:**
- **Hetzner Cloud** (Cheapest): CPX31 - 8GB RAM, 4 vCPU - ‚Ç¨12/month
- **DigitalOcean**: 8GB Droplet - $48/month
- **Linode**: 8GB Plan - $36/month
- **Vultr**: 8GB Plan - $24/month

### Option 2: AWS EC2 üí∞ ~$50-100/month
- t3.large (8GB RAM) or better
- More expensive but professional

### Option 3: Google Cloud Compute üí∞ ~$40-80/month
- e2-standard-4 (16GB RAM) recommended

### Option 4: Docker on Your Own Server üí∞ Free (if you have a server)
- Use existing hardware
- Best for development/testing

---

## Quick VPS Deploy (Recommended) ‚ö°

### Step 1: Get a VPS

**Hetzner Cloud (Cheapest):**
1. Go to https://www.hetzner.com/cloud
2. Create account
3. Choose: **CPX31** (8GB RAM, 4 vCPU) - ‚Ç¨12/month
4. Select location closest to you
5. OS: **Ubuntu 22.04 LTS**

### Step 2: Initial Server Setup

**Connect to server:**
```bash
ssh root@your-server-ip
```

**Update system:**
```bash
apt update && apt upgrade -y
apt install -y git python3-pip python3-venv nginx curl
```

**Create user:**
```bash
adduser allerpredict
usermod -aG sudo allerpredict
su - allerpredict
```

### Step 3: Install Ollama

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Start Ollama service
sudo systemctl start ollama
sudo systemctl enable ollama

# Download model (choose one):
ollama pull phi3:mini        # 3.8GB - Fast
# OR
ollama pull llama2           # 7GB - Better quality
# OR  
ollama pull mistral:7b       # 7GB - Best quality

# Verify
ollama list
```

### Step 4: Deploy Backend

```bash
# Clone your repo
git clone https://github.com/your-username/allerpredict.git
cd allerpredict/backend

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Create .env file
nano .env
```

**Add to .env:**
```bash
HOST=0.0.0.0
PORT=8000
OLLAMA_BASE_URL=http://localhost:11434
```

**Test run:**
```bash
python main.py
```

### Step 5: Setup as Service

**Create systemd service:**
```bash
sudo nano /etc/systemd/system/allerpredict.service
```

**Add:**
```ini
[Unit]
Description=AllerPredict AI API
After=network.target ollama.service
Requires=ollama.service

[Service]
Type=simple
User=allerpredict
WorkingDirectory=/home/allerpredict/allerpredict/backend
Environment="PATH=/home/allerpredict/allerpredict/backend/venv/bin"
ExecStart=/home/allerpredict/allerpredict/backend/venv/bin/uvicorn main:app --host 0.0.0.0 --port 8000
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

**Start service:**
```bash
sudo systemctl daemon-reload
sudo systemctl start allerpredict
sudo systemctl enable allerpredict
sudo systemctl status allerpredict
```

### Step 6: Setup Nginx Reverse Proxy

```bash
sudo nano /etc/nginx/sites-available/allerpredict
```

**Add:**
```nginx
server {
    listen 80;
    server_name your-domain.com;  # or your-server-ip

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # WebSocket support (if needed)
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        
        # Timeout for long AI requests
        proxy_read_timeout 300s;
        proxy_connect_timeout 300s;
    }
}
```

**Enable site:**
```bash
sudo ln -s /etc/nginx/sites-available/allerpredict /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl restart nginx
```

### Step 7: Setup SSL (Free with Let's Encrypt)

```bash
sudo apt install certbot python3-certbot-nginx
sudo certbot --nginx -d your-domain.com
```

**Your API is now live at:** `https://your-domain.com`

---

## Docker Deploy üê≥

### Full Docker Setup

**Create `docker-compose.yml`:**
```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    command: serve

  backend:
    build: ./backend
    container_name: allerpredict-backend
    depends_on:
      - ollama
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - ./backend:/app
    restart: unless-stopped

  frontend:
    build: ./frontend
    container_name: allerpredict-frontend
    ports:
      - "80:80"
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  ollama_data:
```

**Backend Dockerfile:**
```dockerfile
FROM python:3.10-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy app
COPY . .

# Expose port
EXPOSE 8000

# Run
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Frontend Dockerfile:**
```dockerfile
FROM node:18-alpine AS builder

WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .

# Set API URL
ENV REACT_APP_API_URL=http://localhost:8000
RUN npm run build

FROM nginx:alpine
COPY --from=builder /app/build /usr/share/nginx/html
COPY nginx.conf /etc/nginx/conf.d/default.conf
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

**Deploy:**
```bash
# Start all services
docker-compose up -d

# Download model inside Ollama container
docker exec -it ollama ollama pull phi3:mini

# Check logs
docker-compose logs -f

# Stop
docker-compose down
```

---

## Cloud Deploy (AWS EC2)

### 1. Launch EC2 Instance

**Specs:**
- **Instance Type:** t3.large (8GB RAM minimum)
- **AMI:** Ubuntu 22.04 LTS
- **Storage:** 40GB GP3
- **Security Group:** Open ports 22, 80, 443, 8000

### 2. Connect and Setup

```bash
# Connect
ssh -i your-key.pem ubuntu@ec2-ip

# Follow VPS setup steps above
```

### 3. Configure Security

```bash
# Firewall
sudo ufw allow 22/tcp
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp
sudo ufw enable

# Fail2ban (protect SSH)
sudo apt install fail2ban
sudo systemctl enable fail2ban
```

---

## Frontend Deploy Options

### Option 1: Same Server (Simple)

**Build and serve:**
```bash
cd frontend
npm run build

# Copy to nginx web root
sudo cp -r build/* /var/www/html/

# Update API URL in build
# Edit /var/www/html/static/js/main.*.js
# Replace localhost:8000 with your-domain.com
```

### Option 2: Separate CDN (Better Performance)

**Deploy on Vercel/Netlify:**
```bash
cd frontend

# Create .env.production
echo "REACT_APP_API_URL=https://your-api-domain.com" > .env.production

# Build
npm run build

# Deploy to Vercel
npm install -g vercel
vercel --prod

# Or Netlify
npm install -g netlify-cli
netlify deploy --prod
```

---

## Production Setup üîß

### 1. Optimize Ollama Performance

**Edit ollama service:**
```bash
sudo nano /etc/systemd/system/ollama.service
```

**Add environment variables:**
```ini
[Service]
Environment="OLLAMA_NUM_PARALLEL=4"
Environment="OLLAMA_MAX_LOADED_MODELS=1"
Environment="OLLAMA_KEEP_ALIVE=5m"
```

### 2. Monitoring

**Install monitoring tools:**
```bash
# System monitoring
sudo apt install htop iotop

# Check resources
htop

# Monitor Ollama
curl http://localhost:11434/api/tags

# Check logs
sudo journalctl -u allerpredict -f
sudo journalctl -u ollama -f
```

### 3. Backup Script

**Create backup:**
```bash
nano ~/backup.sh
```

**Add:**
```bash
#!/bin/bash
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/home/allerpredict/backups"

mkdir -p $BACKUP_DIR

# Backup code
tar -czf $BACKUP_DIR/code_$DATE.tar.gz /home/allerpredict/allerpredict

# Backup Ollama models
tar -czf $BACKUP_DIR/ollama_$DATE.tar.gz ~/.ollama

# Keep only last 7 days
find $BACKUP_DIR -name "*.tar.gz" -mtime +7 -delete

echo "Backup completed: $DATE"
```

**Make executable and schedule:**
```bash
chmod +x ~/backup.sh
crontab -e
# Add: 0 2 * * * /home/allerpredict/backup.sh
```

### 4. Auto-Update Script

```bash
nano ~/update.sh
```

```bash
#!/bin/bash
cd /home/allerpredict/allerpredict
git pull origin main
cd backend
source venv/bin/activate
pip install -r requirements.txt
sudo systemctl restart allerpredict
echo "Update completed"
```

---

## Performance Tips üöÄ

### 1. Use Smaller, Faster Models

```bash
# Best for production (speed vs quality):
ollama pull phi3:mini        # Fast + Good quality
ollama pull qwen2.5:3b       # Very fast
ollama pull mistral:7b       # Best quality but slower
```

### 2. Adjust Context Window

**In `analysis_agent.py`:**
```python
llm = Ollama(
    model="phi3:mini",
    temperature=0.7,
    num_ctx=2048  # Reduce from 4096 for faster responses
)
```

### 3. Enable Caching

**Add to `main.py`:**
```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_cached_analysis(product_name: str):
    # Cache analysis results
    pass
```

---

## Cost Breakdown üí∞

### Monthly Costs:

**VPS Option (Hetzner):**
- Server (CPX31): ‚Ç¨12/month (~$13)
- Domain: $10-15/year (~$1.25/month)
- **Total: ~$14/month**

**AWS EC2:**
- t3.large: ~$60/month
- Elastic IP: $3.60/month
- Storage: ~$5/month
- **Total: ~$68/month**

**Your Own Server:**
- Hardware cost (one-time)
- Electricity: minimal
- **Total: ~$5-10/month (electricity)**

---

## Troubleshooting üîß

### Issue: Ollama Not Starting
```bash
sudo systemctl status ollama
sudo journalctl -u ollama -n 50
```

### Issue: Out of Memory
```bash
# Check memory
free -h

# Kill unused processes
sudo systemctl stop ollama
sudo systemctl start ollama

# Restart backend
sudo systemctl restart allerpredict
```

### Issue: Slow Responses
```bash
# Use smaller model
ollama pull phi3:mini

# Update analysis_agent.py to use phi3:mini
# Reduce num_ctx to 2048
```

### Issue: Can't Connect to API
```bash
# Check if running
sudo systemctl status allerpredict
curl http://localhost:8000/health

# Check nginx
sudo nginx -t
sudo systemctl status nginx
```

---

## Quick Commands Cheat Sheet üìù

```bash
# Check all services
sudo systemctl status ollama
sudo systemctl status allerpredict
sudo systemctl status nginx

# View logs
sudo journalctl -u allerpredict -f
sudo journalctl -u ollama -f

# Restart services
sudo systemctl restart ollama
sudo systemctl restart allerpredict
sudo systemctl restart nginx

# Update code
cd ~/allerpredict && git pull
sudo systemctl restart allerpredict

# Check Ollama models
ollama list

# Monitor resources
htop
```

---

## Next Steps After Deployment üéØ

1. ‚úÖ Set up domain name
2. ‚úÖ Configure SSL certificate
3. ‚úÖ Set up monitoring (UptimeRobot)
4. ‚úÖ Configure backups
5. ‚úÖ Add error logging (Sentry)
6. ‚úÖ Implement rate limiting
7. ‚úÖ Set up CI/CD (GitHub Actions)

---

## Comparison: Ollama vs OpenAI

| Feature | Ollama (VPS) | OpenAI API |
|---------|-------------|-----------|
| **Cost** | ~$14/month fixed | ~$0.15-2 per 1M tokens |
| **Speed** | 30-60s per request | 3-5s per request |
| **Privacy** | 100% Private | Data sent to OpenAI |
| **Setup** | Complex | Simple |
| **Scaling** | Manual upgrade | Automatic |
| **Best for** | Privacy, low volume | Speed, high volume |

---

**Ready to deploy? Pick your option and I'll guide you step by step! üöÄ**